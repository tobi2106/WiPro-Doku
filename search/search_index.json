{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"WiPro-Kubernetes-Scheduling","text":"<p>Hier einmal unser Projekt vorstellen und unsere Fragestellung n\u00e4her bringen.  </p>"},{"location":"#setup","title":"Setup","text":"<ol> <li>Virtuelles Environment it Multipass</li> <li>Pushen von Docker-Images in ein Kubernetes Namespace</li> </ol>"},{"location":"setup/IdeToVMConnection/","title":"SSH Verbindung in VSC","text":"<p>Den Go Code f\u00fcr unseren Kubernetes Scheduler m\u00f6chte ich in der VisualStudioCode (VSC) IDE auf meinem Windows Rechner schreiben.  Daf\u00fcr wird eine SSH Verbindung von VSC zu der Linux VM ben\u00f6tigt.  Mithilfe der Erweiterung Remote - SSH kann dies leicht erreicht werden.  </p> <p>Mit F1 kann die Befehlsanzeige von VSC aufgemacht werden und nach <code>Remote-SSH: Connect to Host...</code> gesucht werden.  Wenn der Command ausgew\u00e4hlt ist, kann entweder eine neue SSH Host Konfigurationsdatei angelegt werden oder  direkt mit <code>&lt;user&gt;@&lt;host&gt;</code> eine SSH Verbindung aufgebaut werden.  Die File wird standardm\u00e4\u00dfig unter <code>C:\\Users\\&lt;username&gt;\\.ssh</code> gespeichert.  Zuletzt wird die Eingabe im Command best\u00e4tigt und man muss beim Ausf\u00fchren bzw. Verbinden noch im Kontextmenue das Betriebssystem w\u00e4hlen.</p> <pre><code>Host MicroK8s\n  HostName 172.30.169.161\n  User ubuntu\n</code></pre> <p>Sobald die Verbindung steht,  wird unten Links in VSC in Blau der Aktuelle Host angezeigt - in meinem Fall <code>MicroK8s</code>. </p> <p></p>"},{"location":"setup/IdeToVMConnection/#programmierung-im-terminal-mit-neovim","title":"Programmierung im Terminal mit Neovim","text":"<p>Alternativ arbeiten wir direkt in der VM mit Neovim.  Mir pers\u00f6nlich gef\u00e4llt das Arbeiten in VSC besser,  daher bevorzuge ich das oben beschriebene Setup mehr.  Von der Leistung oder Effizienz unterscheiden sich beide Methode nicht,  es ist eine einfache Pr\u00e4ferenz. </p>"},{"location":"setup/Kubernetes/","title":"Kubernetes","text":""},{"location":"setup/Kubernetes/#kubernetes","title":"Kubernetes","text":""},{"location":"setup/dockerImageToNamespace/","title":"Docker Images im Kubernetes Namespace","text":""},{"location":"setup/dockerImageToNamespace/#text","title":"Text","text":"<p>Ich schreibe ein text</p>"},{"location":"setup/microk8s_cluster/","title":"MikroK8s Clusters","text":"<p>Damit wir mit dem Projekt starten k\u00f6nnen und unsere Fragestellung direkt angehen k\u00f6nnen,  wollen wir zuerst ein prove of work Konzept in einem kleinen lokalem MikroK8s Cluster testen. </p>"},{"location":"setup/microk8s_cluster/#einrichten-eines-microk8s-clusters-in-der-vm","title":"Einrichten eines MicroK8s Clusters in der VM","text":"<p>In der Linux VM kann nun mit folgenden Befehlen ein MicroK8s Cluster installiert werden.  Die Dokumentation folgt die Anweisungen aus dem offiziellen MicroK8s Webpage.  <code>$ sudo snap install microk8s --classic</code> <code>$ sudo microk8s status --wait-ready</code> <code>$ sudo microk8s enable dns</code> <code>$ sudo microk8s enable registry</code> <code>$ sudo microk8s enable istio</code> </p> Command Outputs: <pre><code>ubuntu@microk8s-vm:~$ sudo snap install microk8s --classic\n2024-09-25T23:22:39Z INFO Waiting for automatic snapd restart...\nmicrok8s (1.30/stable) v1.30.4 from Canonical\u2713 installed\n</code></pre> <pre><code>ubuntu@microk8s-vm:~$ sudo microk8s status --wait-ready\nmicrok8s is running\nhigh-availability: no\n  datastore master nodes: 127.0.0.1:19001\n  datastore standby nodes: none\naddons:\n    ...\n</code></pre> <pre><code>ubuntu@microk8s-vm:~$ sudo microk8s enable dns\nInfer repository core for addon dns\nAddon core/dns is already enabled\n</code></pre> <pre><code>ubuntu@microk8s-vm:~$ sudo microk8s enable registry\nInfer repository core for addon registry\nInfer repository core for addon hostpath-storage\nEnabling default storage class.\nWARNING: Hostpath storage is not suitable for production environments.\n         A hostpath volume can grow beyond the size limit set in the volume claim manifest.\ndeployment.apps/hostpath-provisioner created\nstorageclass.storage.k8s.io/microk8s-hostpath created\nserviceaccount/microk8s-hostpath created\nclusterrole.rbac.authorization.k8s.io/microk8s-hostpath created\nclusterrolebinding.rbac.authorization.k8s.io/microk8s-hostpath created\nStorage will be available soon.\nThe registry will be created with the size of 20Gi.\nDefault storage class will be used.\nnamespace/container-registry created\npersistentvolumeclaim/registry-claim created\ndeployment.apps/registry created\nservice/registry created\nconfigmap/local-registry-hosting configured\n</code></pre> <pre><code>ubuntu@microk8s-vm:~$ sudo microk8s enable istio\nAddon istio was not found in any repository\nTo use the community maintained flavor enable the respective repository:\n\n    microk8s enable community\n</code></pre> <p>Sobald der CLuster hochgefahren ist  und die Funktionen aktiviert wurden sind,  kann der aktuelle Namespace \u00fcberpr\u00fcft werden:  <code>$ sudo microk8s kubectl get all --all-namespaces</code> </p> <p>Mit dem Befehlen <code>start</code> und <code>stop</code> kann der Cluster wieder gestartet und gestoppt werden.  <code>$ sudo microk8s start</code> <code>$ sudo microk8s stop</code> </p>"},{"location":"setup/microk8s_cluster/#aktivieren-des-microk8s-dashboards","title":"Aktivieren des MicroK8s Dashboards","text":"<p>Alternativ kann noch das MicroK8s Dashboard aktiviert werden.  Dieser muss aber entsprechend aus der VM heraus \u00fcber unser Rechner port forward werden,  um inm Webbrowser angezeigt werden zu k\u00f6nnen. Daf\u00fcr muss ein Token f\u00fcr den Login auf das Dashboard generiert werden.   <code>$ sudo microk8s kubectl create token default</code> <code>$ sudo microk8s enable dashboard</code> <code>$ sudo microk8s dashboard-proxy</code> </p> <p>Damit in Windows die Port-Weiterleitung funktioniert,  muss der Port in der Firewall erlaubt werden. Danach kann im MicroK8s Cluster das Dashboard folgenderma\u00dfen gestartet werden:  <code>$ sudo microk8s kubectl port-forward -n kube-system service/kubernetes-dashboard 10443:443 --address 0.0.0.0</code></p> Error Failed to verify certificate: x509 <p>Ich habe bei dem Ausf\u00fchren des Befehls einen TLS Error bekommen:  \"error upgrading connection: error dialing backend: tls: failed to verify certificate: x509\"   Der Error liegt an abgelaufene Zertifikate,  die die mit dem Befehl <code>$ sudo microk8s.refresh-certs -i</code> eingesehen werden k\u00f6nnen und mit  <code>$ sudo microk8s.refresh-certs -c</code> aktualisiert werden.  Eine genaue Erkl\u00e4rung mit L\u00f6sung ist auf dem Blog von Boris Quiroz beschreiben.</p> <p>Komischer Weise habe ich den Cluster erst vor ein paar Stunden erstellt...</p> <p>Wenn der Proxy f\u00fcr das Dashboard l\u00e4uft und keine Firewall den Port blocked,  dann sieht das Dashboard so aus.  Darauf hin muss lediglich der Token eingegeben werden,  um sich mit dem Cluster zu verbinden. </p> <p> </p> <p></p>"},{"location":"setup/microk8s_cluster/#alternatives-dashboard","title":"Alternatives Dashboard","text":"<p>Alternativ zu dem Dashboard das direkt mit Kubernetes mitkommt,  kann zum Beispiel OpenLens genutzt werden.  OpenLens kam bereits \u00f6fter in einigen Modulen unseres Master Studiums vor und  funktioniert \u00e4hnlich zu dem Kubernetes Dashboard.   </p> <p>OpenLens versteht sich als Kubernetes IDE und  bietet die Funktionen Kubernetes Cluster zu managen, Fehler innerhalb der Cluster ausfindig zu machen und  gleichzeitig eine sehr intuitive UI zu bieten.  An dieser Stelle empfehle ich selbst einfach OpenLens als kostenlose Variante zu Lens.  OpenLens ist vollkommen ausreichend f\u00fcr kleine Projekte wie unser Kubernetes Scheduler.  Auf Windows oder Linux kann einfach unter dem Release Tap des OpenLens Repos eine Installationsfile heruntergeladen werden.</p>"},{"location":"setup/microk8s_cluster/#openlens","title":"OpenLens","text":"<p>F\u00fcr das Hinzuf\u00fcgen eines Clusters zu OpenLens wird die <code>.kubeconfig</code> Informationen des Clusters ben\u00f6tigt.  Als Administrator des Clusters ist dies einfach m\u00f6glich mit dem folgendem Befehl:   <code>$ sudo microk8s config</code></p> <p>Dieser Befehl gibt folgenden Output: </p> <pre><code>apiVersion: v1\nclusters:\n- cluster:\n    certificate-authority-data: \n    &lt;some_hash_value&gt;\n    server: https://172.30.165.61:16443\n  name: microk8s-cluster\ncontexts:\n- context:\n    cluster: microk8s-cluster\n    user: admin\n  name: microk8s\ncurrent-context: microk8s\nkind: Config\npreferences: {}\nusers:\n- name: admin\n  user:\n    client-certificate-data: \n    &lt;some_hash_value&gt;\n    client-key-data: \n    &lt;some_hash_value&gt;\n</code></pre> Mehrere Cluster in einer kubeconfig <p>In der Config stehen alle Informationen und die gehashten Werte f\u00fcr die Verifikation und Authentifikation.  Au\u00dferdem kann mithilfe des <code>current-context</code> den aktuellen Cluster ausgew\u00e4hlt werden.  In meinem Fall habe ich selbst von der Hochschule mehrere Kubernetes Contexts mit verschiedenen Usern laufen.  So k\u00f6nnen also problemlos mehrere Cluster innerhalb einer <code>.kubeconfig</code> File gespeichert werden. </p> <p>Mit den Befehlen k\u00f6nnen der aktuelle Context angezeigt und gewechselt werden:  <code>$ sudo microk8s kubectl config get-contexts</code> <code>$ sudo microk8s kubectl config use-context</code></p> <p>Der Inhalt der <code>.kubeconfig</code> sollte dann zwischengespeichert werden und kann darauf zu Lens hinzugef\u00fcgt werden.  An dieser Stelle sei wieder angemerkt: In unserem Fall wird der Cluster innerhalb einer eigenen VM gehostet. Entsprechend m\u00fcssen alle Ports erlaubt und richtig port forwarded werden, sodass der Windows Rechner auf den Linux VM zugriff hat.</p> <p>Sobald OpenLens gestartet ist,  kann mit dem Shortcut Ctrl+Shift+A ein weitere Cluster anhand der <code>.kubeconfig</code> hinzugef\u00fcgt werden.  Alternativ kann auch auf die Optionsstriche oben Links geklickt werden und darauf auf <code>File</code> und <code>Add Cluster</code>. </p> <p>Nach dem Hinzuf\u00fcgen kann der Cluster an die Hotbar angeheftet werden,  indem man auf das Pin-Symbol klickt,  sobald man mit der Maus \u00fcber das Cluster hovered.</p> <p></p> <p>Verbindet man sich nun mit dem Cluster sieht es folgenderma\u00dfen aus. Ein sehr \u00e4hnlicher Aufbau wie zu dem Kubernetes Dashboard. </p> <p></p>"},{"location":"setup/microk8s_cluster/#vsc-erweiterung","title":"VSC Erweiterung","text":"<p>In VisualStudioCode existiert ebenfalls eine Erweiterung f\u00fcr Kubernetes.  Die Erweiterung Kubernetes  ist von Microsoft geschrieben und bietet ebenfalls sehr \u00e4hnliche M\u00f6glichkeiten wie das Dashboard oder Lens. </p> <p>Damit dies funktioniert muss die <code>.kubeconfig</code> File aus dem Cluster ausgelesen werden und  zu der Erweiterung hinzugef\u00fcgt werden.  Nach der Installation der Erweiterung erscheint ein Kubernetes Icon.  In dem Tab kann \u00fcber die drei Einstellungspunkte ein existierendes Cluster hinzugef\u00fcgt werden. Falls man selbst eine <code>.kubeconfig</code> File mit mehren Clustern h\u00e4lt,  kann diese ebenfalls angegeben werden.</p> <p></p>"},{"location":"setup/virtuelEnvionment/","title":"Virtuelles Environment","text":"<p>Das MicroK8s Cluster wird in einer VM von Multipass aufgesetzt.  </p>"},{"location":"setup/virtuelEnvionment/#erstellen-einer-multipass-instanz","title":"Erstellen einer Multipass Instanz","text":"<p>F\u00fcr das Aufsetzen eines MicroK8s Clusters wird auf der offiziellen MicroK8s Homepage Multipass empfohlen.  F\u00fcr die Windowsinstallation wird direkt auf ein Installer verlinkt,  der ein existierenden MicroK8s Cluster aufsetzt.  Es wird eine VM mit 4 CPUs, 4G RAM und 40 GB an Speicher.  F\u00fcr unsere Testzwecke sind die Ressourcen-Empfehlungen mehr als Ausreichend und  haben entsprechend jeweils nur die H\u00e4lfte zugewiesen. </p> <p></p> <p>Nachdem die VM hochgefahren ist,  direkt \u00fcber die Multipass GUI Shell mit der VM interagiert werden,  oder man kann direkt \u00fcber ein Terminal mit der Instanz interagieren:   <code>$ multipass start &lt;name&gt;</code> <code>$ multipass shell &lt;name&gt;</code> <code>$ multipass restart &lt;name&gt;</code> </p> <p>F\u00fcr das erstellen einer neuen Instanz kann ebenfalls das Terminal verwendet werden:   <code>$ multipass launch -n &lt;name&gt; -c &lt;cpus&gt; -m &lt;memory&gt; -d &lt;disk space&gt;</code></p> <p>Es kann noch ein <code>--image</code> Argument \u00fcbergeben werden, um ein spezifisches Image auszuw\u00e4hlen,  als Standard wird das neuste Ubuntu LTS System genutzt.  Wir arbeiten mit der Ubuntu Version 24.04.01. </p> <p>Sobald die VM hochgefahren ist,  wird eine SSH Verbindung in der VM eingerichtet,  um sich auch au\u00dferhalb der VM mit dem Cluster zu verbinden. </p>"},{"location":"setup/virtuelEnvionment/#setup-der-ssh-verbindung","title":"Setup der SSH Verbindung","text":"<p>Zuerst muss apt geupdated werden und darauf der openssh-client installiert werden.  <code>$ sudo apt update &amp;&amp; sudo apt upgrade</code> <code>$ sudo apt install openssh-client</code> </p> <p>Danach wird \u00fcberpr\u00fcft ob die ssh-Verbindung steht:   <code>$ sudo systemctl status ssh</code></p> <p></p> <p>Somit l\u00e4uft die SSH Verbindung auf der VM: <code>Active: active (running) since Wed 2024-09-25 14:32:27 UTC; 2min 19s ago</code>.</p> <p>In unserem fall m\u00fcssen wir ssh in der Firewall noch aktiviren:   <code>$ sudo ufw allow ssh</code> <code>$ sudo systemctl enable ssh</code> </p> <p>Danach muss die IP Adresse der Linux-VM mit <code>ifconfig</code> ausgelesen werden:  <code>$ sudo apt install net-tools</code> <code>$ ifconfig</code> </p> <pre><code>eth0: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt;  mtu 1500\n        inet 172.30.169.161  netmask 255.255.240.0  broadcast 172.30.175.255\n        inet6 fe80::5054:ff:fee3:86bf  prefixlen 64  scopeid 0x20&lt;link&gt;\n        ether 52:54:00:e3:86:bf  txqueuelen 1000  (Ethernet)\n        RX packets 4606  bytes 1326666 (1.3 MB)\n        RX errors 0  dropped 0  overruns 0  frame 0\n        TX packets 4631  bytes 564837 (564.8 KB)\n        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0\n</code></pre> <p>In unsrem Fall ist die IP: <code>172.30.169.161</code>. Somit kann sich \u00fcber die IP und dem Username des Linux Rechners mit der VM verbunden werden: <code>ssh ubuntu@172.30.169.161</code>. </p>"},{"location":"setup/virtuelEnvionment/#ssh-authentifizierung","title":"SSH-Authentifizierung","text":"<p>Damit die SSH-Verbindung aufgebaut werden kann,  muss entweder ein Username mit Password auf der VM Instanz gesetzt werden,  oder man verbindet sich \u00fcber die Public Key Authentifizierung.  Wir haben uns f\u00fcr das letzte entscheiden,  um einerseits mehr mit SSH zu arbeiten und dies zu \u00fcben,  als auch f\u00fcr eine sichere und schnellere Verbindung.  Sobald die Keys gesetzt sind,  kann die Verbindung ohne Login aufgebaut werden.   <code>$ ssh-keygen</code></p> <p></p> <p>Standardgem\u00e4\u00df wird der SSH Key unter <code>/&lt;your_home/.ssh/id_rsa&gt;</code> gespeichert. Au\u00dferdem kann ein Passphrase f\u00fcr die Keys angegeben werden.  Danach muss in der VM noch der \u00f6ffentliche Schl\u00fcssel von den Rechnern hinzugef\u00fcgt werden,  die sich mit der VM verbinden wollen. </p>"},{"location":"setup/virtuelEnvionment/#openshh-installation","title":"OpenSHH Installation","text":"<p>In meinem Fall m\u00f6chte ich mich von einem Windows Rechner in die VM Verbinden.  Entsprechend habe ich mit OpenSSH ebenfalls Schl\u00fcssel generiert.  Das Vorgehen funktioniert sehr \u00e4hnlich und wird auf der Microsoft Seite Erste Schritte mit OpenSSH f\u00fcr Windows erkl\u00e4rt:  In PowerShell: <code>$ Get-WindowsCapability -Online | Where-Object Name -like 'OpenSSH*'</code> <code>$ Add-WindowsCapability -Online -Name OpenSSH.Client~~~~0.0.1.0</code> <code>$ Add-WindowsCapability -Online -Name OpenSSH.Server~~~~0.0.1.0</code> </p> <p>Output: <pre><code>Path          :\nOnline        : True\nRestartNeeded : False\n</code></pre> <code>$ Start-Service sshd</code> <code>$ Set-Service -Name sshd -StartupType 'Automatic'</code> </p> <p>Zuletzt m\u00fcssen noch die Ports in der WindowsFirewall freigegeben werden:  <pre><code>if (!(Get-NetFirewallRule -Name \"OpenSSH-Server-In-TCP\" -ErrorAction SilentlyContinue | Select-Object Name, Enabled)) {\n    Write-Output \"Firewall Rule 'OpenSSH-Server-In-TCP' does not exist, creating it...\"\n    New-NetFirewallRule -Name 'OpenSSH-Server-In-TCP' -DisplayName 'OpenSSH Server (sshd)' -Enabled True -Direction Inbound -Protocol TCP -Action Allow -LocalPort 22\n} else {\n    Write-Output \"Firewall rule 'OpenSSH-Server-In-TCP' has been created and exists.\"\n}\n</code></pre></p> <p>Jetzt kann in einem Windows-Terminal mit <code>$ ssh-keygen -t rsa</code> ebenfalls ein Schl\u00fcsselpaar generiert werden. </p>"},{"location":"setup/virtuelEnvionment/#hinzufugen-eines-offentlichen-ssh-schlussel-in-linux","title":"Hinzuf\u00fcgen eines \u00d6ffentlichen SSH Schl\u00fcssel in Linux","text":"<p>Der \u00f6ffentliche Schl\u00fcssel von dem Rechner,  der sich nun mit der Linux Instanz verbinden m\u00f6chte,  muss kopiert werden und auf dem Linux Rechner hinterlegt werden.  Daf\u00fcr muss das <code>.ssh</code> Verzeichnis in die File <code>authorized_keys</code> der \u00f6ffentliche Schl\u00fcssel hinzugef\u00fcgt werden und  die Rechte auf das File entsprechend gesetzt werden.  <code>$ echo \"&lt;PUBLIC_KEY&gt; &gt;&gt; ~/.ssh/authorized_keys\"</code> <code>$ chmod 600 ~/.ssh/authorized_keys</code> </p> <p>Danach muss lediglich ssh neugestartet werden und es kann versucht werden, eine SSH-Verbindung aufzubauen:  <code>$ sudo systemctl restart ssh</code></p> <p>Auf meinem Windows Rechner:</p> <pre><code>C:\\Users\\Tobby\n\u03bb ssh ubuntu@172.30.169.161\nThe authenticity of host '172.30.169.161 (172.30.169.161)' can t be established.\nECDSA key fingerprint is SHA256:5cuD09ZKcsANuY9RW5kp0gI+34fzbS/UYTxsNSx3+10.\nAre you sure you want to continue connecting (yes/no/[fingerprint])? yes\nWarning: Permanently added '172.30.169.161' (ECDSA) to the list of known hosts.\nWelcome to Ubuntu 24.04.1 LTS (GNU/Linux 6.8.0-45-generic x86_64)\n\n * Documentation:  https://help.ubuntu.com\n * Management:     https://landscape.canonical.com\n * Support:        https://ubuntu.com/pro\n\n System information as of Wed Sep 25 15:42:36 UTC 2024\n\n  System load:  0.08               Processes:             108\n  Usage of /:   10.2% of 18.33GB   Users logged in:       1\n  Memory usage: 11%                IPv4 address for eth0: 172.30.169.161\n  Swap usage:   0%\n\n\nExpanded Security Maintenance for Applications is not enabled.\n\n0 updates can be applied immediately.\n\nEnable ESM Apps to receive additional future security updates.\nSee https://ubuntu.com/esm or run: sudo pro status\n\n\nLast login: Wed Sep 25 14:34:05 2024 from 172.30.160.1\n</code></pre>"}]}